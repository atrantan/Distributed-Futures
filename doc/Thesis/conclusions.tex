\chapter{Conclusions and Future Work}
\label{chap:conclusion}
\paragraph{}
In this work we presented an implementation of the futures programming model as a C++ library,
for distributed and shared memory machines.  We implemented our system using the MPI one-sided communication
library for communication. We adopted shared memory scheduling techniques to implement 
our scheduler, making use again of the MPI one-sided interface.  In terms of interface, we showed that it 
is possible to implement the shared memory C++11 standard interface.  There are only minimal differences
between the two interfaces, none of which limits the the expressiveness and usability of the futures model. 
Our performance evaluation of the system shows
that the current implementation suffers from significant overheads, making it unsuitable for fine grain
parallelization.  For the most part, the high overhead can be mainly attributed to the serialization 
routines and the MPI based mutex library we use.  

\paragraph{}
At this point, from our experience with the MPI one-sided communication interface, we believe that there exist
some fundamental limitations in its design. These are:
\\
\begin{enumerate}
	\item MPI\_Window creation is a collective operation over a group of MPI processes.  In order to dynamically
				allocate data and share through a window, all processes must synchronize, calling the MPI\_Window\_create
				routine.  For our asynchronous system this is a serious limitation, especially when we only want to create
				windows between only two processes at a time.  The only solution to this problem would be to create a priori
				all possible groups for all pairs of processes, which can be costly.  Instead, we were forced to preallocate
				a buffer for each process, that is shared through a window.
	
	\item The \emph{active mode} "epoch" definition scheme, requires both processes to take part in the communication,
				which we believe to be counter intuitive for an one-sided communication interface. What's more, we find that
				it is unusable in our asynchronous communication system.

	\item The locking schematics of the \emph{passive mode} "epoch" definition scheme, do not define well what happens
				when a window is concurrently accessed, which can cause erroneous results.  This forced us to implement our
				own mutexes to synchronize data accesses on the same window.  Moreover, acquiring an exclusive lock on a 
				window will block other processes from accessing it, even if they access different, non-overlapping addresses
				in that window.  The later constraint, limits fine grain locking.  In our system, this is a very common
				scenario, where processes, different asynchronous \emph{jobs}, need to write to different parts of the same
		 		window of the process owning the associated futures.

	\item The lack of synchronization primitives, with the same schematics as their shared memory counterparts 
				limits the usability of the model.  Native implementations of such primitives, could offer much better
				performance than implementing them on top of the MPI library.   
\\
\end{enumerate}

\paragraph{}
In the future we plan to address the performance issues of our system.  Currently we use the Boost serialization
library, which is not tuned for performance.  We could try alternative serialization routine that could possibly
match our needs.  The overhead caused by the mutex library though, is tougher to address, while still using 
the one-sided communication Interface.  Unless MPI will not provide a native mutex implementation, alternative
one-sided communication libraries, like ARMCI, should be used.  Less high profile goals include implementing 
all the secondary features of the C++11 futures library.  We are also interested in exploring the potential of having 
a hybrid model with a unified interface. \emph{Jobs} that run on the same machine will use a shared memory runtime, 
while \emph{jobs} that run on different machines, will have to make use of the distributed memory runtime.  
Our main goal is to deliver a high performance runtime system.

