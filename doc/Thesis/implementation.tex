\chapter{Design and Implementation}
\label{chap:implementation}

\paragraph{}
	In this chapter we describe the interface and the implementation details of our futures library.
Our interface allows the programmer to issue callable objects, referred to as \emph{jobs} from now on,
to be executed asynchronously by other processes.  In C++ a callable object is any language struct that
can be treated as a function (function pointers, functors etc). 
An asynchronous call of such an object will return a future object instead of its normal 
return value.  This future object can be used by any other process to retrieve the encapsulated value.  If
the functor has not been executed yet, the reference to the future's value will block \footnote{
as we'll see in section \ref{sect:scheduler}, only the master process blocks while other processes will try to 
run any functor objects that are scheduled to be run.}
until it becomes available.  

\begin{figure}[!ht]
\center
\includegraphics[width=0.5\columnwidth]{figures/system_overview}
\caption{Overview of our futures library design.}
\label{fig:system_overview}
\end{figure} 

\paragraph{}
		We designed our system to be modular, so that different aspects of the runtime library, such as process
communication, hide its underlying implementation (e.g. MPI),  thus different implementations 
of the same module should not interfere with other components of the library.
Figure~\ref{fig:system_overview} shows the different component hierarchy.\\
\\
Our system consists out of three main modules:
\begin{itemize}
	\item The \textbf{communication} module, which is the backbone of the system
	and used by all other components in order to exchange messages and create a shared address space.

	\item The \textbf{Shared Memory Manager}, which is an allocator for the shared address space between the processes.

	\item The \textbf{Scheduler}, which is responsible of handling how \emph{jobs} are send/received between processes
	and also decides which process will run a \emph{job}.
\end{itemize}

\paragraph{}
All the above modules are initialized, finalized and managed by a system environment, 
an instance of which is present at every process.  Note that it is not necessary for all
processes to keep identical environments, which means that other than the initialization
and finalization, processes are only responsible for their local environment, no updates
are necessary.

\begin{figure}[!ht]
\begin{lstlisting}
class helloWorld {
public:
	helloWorld() {};
	~helloWorld() {};
	int operator()() { 
		int id = Futures_Id();
		cout << "- Worker" << id << ":Hello Master" << endl;
		return id;
	};
};

FUTURES_SERIALIZE_CLASS(helloWorld);
FUTURES_EXPORT_FUNCTOR((async_function<helloWorld>));

int main(int argc, char* argv[]) {
	Futures_Initialize(argc, argv);
	helloWorld f;
	future<int> message = async(f);

	cout << "- Master :Hello " << message.get() << endl;

	Futures_Finalize();
};
\end{lstlisting}
\caption{
A simple hello world implementation using the distributed futures interface.  
The output of the program on process 0 would be "- Master :Hello 1".}
\label{lst:hello}
\end{figure}

\begin{figure}[!ht]
\includegraphics[width=\columnwidth]{figures/hello_flow}
\caption{
The control flow of the hello world program in figure~\ref{lst:hello}.}
\label{fig:helloCF}
\end{figure}

\paragraph{}
Figure~\ref{fig:helloCF} shows the program flow for processes 0 and 1, of the simple
hello world example in figure~\ref{lst:hello}.  Before any call to the library is made, the futures
environment must be initialized, which in turn initializes all other library modules 
(e.g. communication, scheduler, memory manager).
All processes execute the main function, but only
the master process will return from it and continue with the user program execution.
All other processes will run our runtime's scheduler code and wait to receive \emph{jobs}.
The \emph{async} function can be called from any process 
and within other async calls, thus allowing recursive algorithms to be expressed. 
In the example, process 0 issues a \emph{job} by
calling \emph{async(f)}.  It will then return from the call and continue until
the message.get() call, at this point the process will either retrieve the message value or block until
it's set.

\paragraph{}
The job is then scheduled to be executed by process 1.  The worker process,
here process 1, will wait until a \emph{job} is send and then run it.  When done, it will set 
the future's value and resumes waiting for other jobs or until it is terminated by the master
process.  When process 0 retrieves message's value, it prints it and continues until it reaches
the Futures\_Finalize() routine.  At this point it will signal all other processes that the program
has reached it's termination point and finalize the futures environment.  All other processes will
do the same after receiving the terminate signal.

\paragraph{}
In the rest of this chapter, we will present the future interface in section~\ref{sect:futures-interface}
and discuss our implementations of the the communication, shared memory manager and scheduler modules in
sections~\ref{sect:communication},~\ref{sect:shared-memory-manager} and~\ref{sect:scheduler}
respectively.

\section{Futures Interface}
\label{sect:futures-interface}

\paragraph{}
An important goal of this work is to provide a unified interface for both distributed and shared memory machines.
To meet this end we decided to replicate the C++11 futures interface from the standard threads library, with which
the C++ community is already familiar and works well with generic programming.  We had to make some additions to the 
interface and impose some restrictions, but they do not limit the capacity of the programmer to express parallelism,
while still keeping the interface as simple as possible to use (we will discuss them shortly). Figure~\ref{lst:fib} 
shows a recursive implementation of the fibonacci function using our future implementation.  The user can issue 
callable objects to be run asynchronously by other processes, using the \emph{async} function and passing the 
callable object, along with its arguments, as the \emph{async}'s arguments.  Note, that in our implementation, the 
callable object can only be a functor object. No normal functions, or function pointers, etc can be used here, which
is the major restriction our implementation has, compared to the C++11 standard library.  The reason for this is that
we send the callable object through the communication module, using messages.  We do not use a compiler, or require
the programmer to identify the functions that are to be run asynchronously and provide a mapping of them to all 
processes.  Instead, we serialize the functor object and send through the message passing library.  

\paragraph{}
This restriction
implies the one major limitation of our interface, which is that the functor object, as well as all of its arguments,
must be serializable.  Back to our example in figure~\ref{lst:fib}, note that \emph{fib1} and \emph{fib2} are both
functor objects.  The user can either provide the serialization routines himself (see~\cite{Ramey:2004:Boost:Serialization} 
for more details on how to serialize a C++ object with Boost serialization library
\footnote{We use the boost serialization library~\cite{Ramey:2004:Boost:Serialization} 
and the input/output archives from the boost mpi library~\cite{Gregor:2005:Boost:MPI}}.), or use the \texttt{FUTURES\_SERIALIZE(F)},
here F is a functor object, which will create the necessary serialization routines automatically.  The former is only 
recommended for very simple functors that have to state (members).  Moreover, the user needs to expose the functor 
type to the underlying serialization library.  This is done with the macro command 
\texttt{FUTURES\_EXPORT\_FUNCTOR(async\_function<fib, int>)} in our example.  Here the type declaration must be wrapped in the
\texttt{async\_function} type, which is the library internal template class for all \emph{jobs}.  The template type 
here is the functor type, fib, and the argument types that will be passed when calling the functor, here \texttt{int}. 
Instantiation of the \texttt{async\_function} class, using C++ templates meta-programming capabilities,
generates the appropriate routines, for setting the future's value, according to the functor's return type.
It also facilitates all necessary information that are needed to be transferred to the worker process.
Figure ~\ref{lst:async_function} shows the definition of the \texttt{async\_function} class.

\paragraph{}
A call to the \emph{async} function is non-blocking and returns a \texttt{future<T>} object immediately, 
where \texttt{T} is the return type of the encapsulated functor object, passed to \emph{async}.
 If the return value is an array, a pointer or any other form of container, the user
should instead call a variation of the \emph{async} function, \texttt{async(N, F, Args...)}, 
where \texttt{N} is number of elements that will be returned 
%\footnote{  
%In section~\ref{FIXME} we discuss an expansion to
%our implementation that would allow us to keep the original interface intact, without the need for
%the extra size argument, but we have not yet implemented it and there are also some performance concerns.}.
In order to retrieve the value,
the owner of the future needs to call the \texttt{get()} method.  This method is blocking, so calling it will cause the 
process to block until the value of the future becomes available.  Alternatively, the future owner can call the
\texttt{is\_ready()} method, which is none blocking, to check if the value can be retrieved, and if not continue running
user code until the future's value becomes available at a later point.  Also, note that before using
the futures library, the user has to explicitly call the \texttt{Futures\_Initialize()} and \texttt{Futures\_Finalize()},
which will initialize and finalize the futures environment, respectively. 
    
\begin{figure}[!ht]
\begin{lstlisting}

template<typename F, typename... Args>
class async_function : public _job {
... //we have ommited here all the serialization routines
public:
		int src_id;
		int dst_id;
		Shared_pointer ptr;
		int data_size;
		int type_size;
    F f;
  	std::tuple<Args...> args;
    typename std::result_of<F(Args...)>::type retVal;
    async_function();
    async_function(int _src_id, int _dst_id, 
									Shared_pointer _ptr, 
									int _data_size, int _type_size,
									F& _f, Args... _args);
    ~async_function();
    void run();
};
\end{lstlisting}
\caption{The \texttt{async\_function} function class definition.  All \emph{jobs} in our system are
instances of this class.  The base class \texttt{\_job} is used for serialization purposes as well.}
\label{lst:async_function}
\end{figure}

\begin{figure}[!ht]
\begin{lstlisting}
class fib {
public:
	fib() {};
	~fib() {};
	int	operator()(int n) {
		if(n == 0) return 0;
		if(n == 1) return 1;
		fib f;
		future<int> fib1 = async(f, n-1);
		future<int> fib2 = async(f, n-2);
		return fib1.get() + fib2.get();;
	};
};

FUTURES_SERIALIZE_CLASS(fib);
FUTURES_EXPORT_FUNCTOR((async_function<fib, int>));

\end{lstlisting}
\caption{A fibonacci implementation using the distributed futures interface}
\label{lst:fib}
\end{figure}



\section{Communication}
\label{sect:communication}

\paragraph{}
The communication module is responsible for message exchange between all of the processes in our system,
as well as providing the infrastructure for a shared address space.
In our implementation the communication module uses MPI-2'S one-sided communication library and Boost MPI's
input and output archives, for object serialization.

\paragraph{}
The communication module acts as a layer of abstraction between the various system component and the MPI library.
It acts as a simple wrapper for initializing, finalizing MPI and simple send/receive operations.
It is also capable of providing information of the MPI environment to the other components of our system (e.g.
number of process, rank e.t.c.).  Moreover, it can be used to expose part of a process' address space to other 
processes in the same communication group.  

\subsection{Shared Address Space}
\paragraph{}
In our implementation, the underlying message passing library used is MPI-2, thus we use MPI windows to
expose such space among processes.  
Exposing part of process' address space in the MPI-2 schema, requires that the some
space will be locally allocated to a pointer using the \texttt{MPI\_Alloc\_mem}, and then exposed to other processes
through creating an MPI window that is correlated to the pointer with \texttt{MPI\_Create\_Win} 
(See section~\ref{sect:mpi-one-sided}). 
A drawback in MPI is that a window can be created only collectively over an MPI communicator,
and in turn, a communicator can be created, again, only collectively over an existing parent communicator. 
In our design, this requires that either all windows are created a priori at initialization, 
since when issuing a job, only the sender and receiver should take part in the communication.  
In order to overcome this limitation, we implemented the algorithm presented in
~\cite{Dinan:2011:NCC:2042476.2042508}, which requires only the processes that will join the communicator to
take part in the communicator creation process.  The algorithm needs an MPI group as input and progressively
creates two adjacent groups of processes.  If a process' id is even, then the process is added to the \emph{right}
group, if the process' id is odd it is added to the \emph{left}.  Every time a process is added to either group, an
inteprocess communicator is created and then merged with the adjacent group's interprocess communicator. The 
algorithm's pseudocode can be found on~\cite[p.287~]{Dinan:2011:NCC:2042476.2042508}.
Employing this algorithm we can dynamically allocate windows between any two processes that compose an MPI group.

\begin{figure}[!ht]
\begin{lstlisting}
void set_data(void* val, int dst_id, Shared_pointer ptr, 
							Datatype datatype) {

    MPI_Win_lock(MPI_LOCK_EXCLUSIVE, dst_id, 0, 
								shared_space[ptr.page_size]);

    MPI_Put(val, ptr.size, datatype, dst_id, ptr.base_address, 
						ptr.size,	datatype, shared_space[ptr.page_size]);

    MPI_Win_unlock(dst_id, shared_space[ptr.page_size]);
};

void set_data(boost::mpi::packed_oarchive& ar, int dst_id, 
							Shared_pointer ptr) {

    MPI_Win_lock(MPI_LOCK_EXCLUSIVE, dst_id, 0, 
								shared_space[ptr.page_size]);

    MPI_Put(&ar.size(), 1, MPI_INT, dst_id, ptr.base_address,
						1, MPI_INT, shared_space[ptr.page_size]);

    MPI_Put(ar.address(), ar.size(), MPI_PACKED, dst_id, 
						ptr.base_address+DATA_OFFSET,
						ar.size(), MPI_PACKED, shared_space[ptr.page_size]);

    MPI_Win_unlock(dst_id, shared_space[ptr.page_size]);
};
\end{lstlisting}
\caption{The function used to set a future's value.  The first version is for primitive data types, 
where as the second is for serializable objects.}
\label{lst:set_data}
\end{figure}

\paragraph{}
The communication library also provides the routines needed to write and read data from an address space shared
though an MPI window, using the special \texttt{Shared\_pointer} construct (see section~\ref{sect:shared-memory-manager}).  
This pointer keeps information of where the data is located within an MPI window in addition to the total size of the data associated with this pointer during its allocation.  Figure~\ref{lst:set_data} shows a simplified version for 
setting a future's value.  The \texttt{ptr} variable has information on the location we need to write the data to on an MPI 
window.  The \texttt{shared\_space[ptr.page\_size]} is a map that contains MPI windows.  Section
~\ref{sect:shared-memory-manager} explains how MPI windows are organized in this map, according to the 
page sized used during allocating space for a future.  Note that the variable datatype, \texttt{MPI\_Datatype} in this
implementation, is inferred statically using template routines from the Boost MPI library,
when instantiating the \texttt{async\_function} class.  The second overloaded \texttt{set\_data} method is used for
when the future's value is not a primitive data type and requires serialization.  In the latter scenario, we
need to store information on the archives size, thus the actual data is indexed at location 
\texttt{ptr.base\_address+DATA\_OFFSET}.

\subsection{Mutexes}
\paragraph{}
In order to synchronize accesses to shared memory addresses and other critical sections in our system, designed
a mutex library, with the same interface as the standard C++ mutex library, which is implemented
for shared memory.  The only difference is that a call to lock, unlock or try\_lock requires the user to specify
the id of the target process. We have adopted MPICH's implementation of mutexes in our design.  A mutex is a
shared vector through an MPI window.  Each vector element is a byte value corresponding to one process.  When
a process wants to hold the mutex lock, it sets its vector value at one and iterates through the rest of the 
vector to check if another process wants or has acquired the lock.  If the lock is acquired or another process 
waits for it, then the current process blocks until it receives a message.  When unlocking, a process sets its 
vector value to zero and then iterates through the vector to find and send a message to next process that is 
waiting to acquire the lock.  

\section{Shared Memory Management}
\label{sect:shared-memory-manager}

\paragraph{}
The Memory Manager module is responsible for managing the systems shared address space. 
It uses the communication module to create address spaces that
are visible by all processes in our system and use the \texttt{Shared\_pointer} construct to describe a location in such
shared memory.  This modules provides the functionality of allocating and freeing space, from the shared address 
space among all processes.  Our allocator is implemented using free lists in order to track free space as 
described in~\cite[p.~185-187]{Tanenbaum:2007:MOS:1410217}.  
However, we keep different free lists for different page sizes to deal
with memory segmentation. Figure~\ref{fig:freeLists} shows how the memory manager keeps a map of free lists
indexed by a memory page size.
The shared address space is allocated a priori
using the communication module, to create MPI windows in our current implementation.  This is of-course transparent to
the Shared Memory Manager module, since it uses Shared\_pointers to describe memory location, size etc.  
The \texttt{Shared\_pointer} is a tuple  \texttt{ptr<ID, BA, SZ, PSZ, PN, ASZ>}, where \texttt{ID} is the id of the process
whose address space we want to address, \texttt{BA} is the base address that the data is located in a shared address space,
\texttt{SZ} is the size of the data we want to allocate, \texttt{PSZ} is the page size the allocator used to allocate for this data,
\texttt{PN} is the number of pages used and \texttt{ASZ} is the the actual size, which is \texttt{PN*PSZ}.  
The information tracked by a 
\texttt{Shared\_pointer} can be effectively used by the communication module to read/write data.  The Shared Memory Manager
modules simply holds a mapping of the shared address space, the actual local addresses are handled by the 
underlying communication library (MPI in our case).   
So, each freeList in figure~\ref{fig:freeLists} is actually a list of 
\texttt{Shared\_pointers} to a corresponding MPI window.  We choose
to keep separate windows for each free list because when acquiring an epoch access to an MPI window, the whole 
window is locked, so even though we do not have overlapping accesses \footnote{only one process needs to write to a 
future's shared address, since only one future is associated with one \emph{job}.} 
(see section~\ref{sect:mpi-one-sided}).  

\begin{figure}[!ht]
\includegraphics[width=\columnwidth]{figures/free_lists}
\caption{Shared Memory Manager keeps a map of free lists, indexed by the page size.  For page size that do
not match any predefined ones, we use the \emph{other} page size free list.}
\label{fig:freeLists}
\end{figure}

\subsection{Memory Allocation/Deallocation}
\paragraph{}
When a process issues an \emph{async} function, it needs to allocate space in its shared address space, for the worker process to store the future's value.  To allocate such space, the host process makes use of the shared memory manager module. 
The shared memory allocator tries to find the best page size
fit for the data size (the one that is closest to the data's total size), and searches the corresponding free list, 
using a first fit algorithm~\cite[p.~185-187]{Tanenbaum:2007:MOS:1410217} to find a large 
enough space for the new data. If no fitting page size is found then the allocator uses a special freeList, 
which does not use a predefined page size, but instead uses the data size to find free space.  If not enough free
space is found in the correct free list, then the allocator can try to find data in another free list, of different
page size.  Figure~\ref{fig:alloc} shows a free list, before and after allocating a data object.
The first fit algorithm 
will iterate the list from the start until it finds a large enough space for the object.  Each node in the free list,
is a \texttt{Shared\_pointer}, which describes how much continuous space there is available.  When the allocator finds a large
enough node, it removes from that node the size and number of pages it needs and sets its base address value 
accordingly.
It then returns a new \texttt{Shared\_pointer}, that describes the memory space that will be now occupied from the data object.
In the example in figure~\ref{fig:alloc}, the first list node has enough space to fit an 128 size data object.  
Removing the reserved now space, from the beginning of the list, will leave us again with two free nodes, but the
first one will now have 512 bytes left and the base address will be moved at the 128th byte.

\begin{figure}[!ht]
\includegraphics[width=0.6\columnwidth]{figures/alloc}
\caption{During allocation, when a large enough space is found, the allocated page is removed from the node.}
\label{fig:alloc}
\end{figure}

\begin{figure}[!ht]
\includegraphics[width=0.7\columnwidth]{figures/free}
\caption{By freeing data at base pointer 512, creates a continuous space between base pointer 0 and
base pointer 640, causing the list nodes to merge into one.}
\label{fig:free}
\end{figure}

\paragraph{}
As soon as a process retrieves a future value, it makes a local copy of it, and frees any shared address space
that is associated with the future.  In order to free shared space, a process needs to provide the Shared\_pointer
that was returned by the allocator routine.  The Shared\_pointer keeps information of the page size used to 
allocate space, thus finding the correct free list is trivial, we just need to use the page size as an index.
We then insert the Shared\_pointer in the free list in a sorted fashion, using the base address for comparison.
This way, all free lists are sorted lists of Shared\_pointers by base address, so that if we find continuous
space, we merge the list elements, resulting in larger block of free space.  Figure~\ref{fig:free} shows a 
free list, before and after freeing some shared memory.  Because freeing 128 bytes at base address 512 creates
a continuous space from byte 0 to byte 640, the two list nodes will be merged into one.

\section{Scheduler}
\label{sect:scheduler}

\paragraph{}
In order to have a distributed memory interface similar to the shared memory one, we chose to implement 
a scheduler, which is responsible for deciding who will execute which \emph{job}.  If the user was 
responsible for distributing \emph{jobs} among the processes,  he would need to reason about dependencies
between \emph{jobs} and retrieving future values, else the program could easily end up in a deadlock.

\begin{figure}
\centering
\includegraphics[width=.7\textwidth]{figures/fib_tasks}
\caption{	\emph{Job} stacks for running fib(3) on the left and fib(5) on the right.  Matching colors denote that
					\emph{job}s are spawned from the same recursion path.}
\label{fig:fib_tasks}
\end{figure}

\paragraph{}
To make our case clear, consider the fibonacci example in figure~\ref{lst:fib}.  
In our example, let's say we have
3 processes, one of them is the master process.  
Figure \ref{fig:fib_tasks} shows how the \emph{job} stacks would look like, for running fibonacci with argument 3 and 5
respectively. The arrows show how \emph{job}s depend upon each other.  A \emph{job} blocks and waits until the \emph{job}
that the arrow points to finishes. Running fib(3), process 0, the master process 
issues \texttt{async(f, 1)} to process 1 and \texttt{async(f, 2)} to process 2.  Process 2 issues \texttt{async(f, 1)} to 
process 1.  In this scenario the program will execute correctly without any
problems, since any of the processes' call to \texttt{get()}  will eventually retrieve the future value.  
But  consider we want to compute \texttt{fib(5)}.  Process 1 may have to run \texttt{async(f, 4)} while process 2 will have to run
\texttt{async(f, 3)}.  At some point, process 1 issues \texttt{async(f, 3)} to process 2, while process 2 issues an 
\texttt{async(f, 2)}
to process 1.  Both processes will return from the \emph{async} calls and proceed calling \texttt{get()} to retrieve the value
but will actually block forever, since neither process will be able to resolve the dependencies as shown in figure 
\ref{fig:fib_tasks}.  This scenario 
is not a problem if processes are dynamically spawned, but if we have a static number of processes, which is
common for MPI programs, we need to address this issues.  

\begin{figure}[!ht]
\center
\includegraphics[width=0.5\columnwidth]{figures/task_stack}
\caption{Shared stack where a worker process keeps its pending jobs.  Entries can have varied sizes, this
size is stored at the beginning of the entry and can be used to retrieve the corresponding job.  Information
for the specific stack,like size and head, are stored at the beginning of the shared space, so that other
processes can access them.}
\label{fig:task_stack}
\end{figure}

\paragraph{}
Since it is not always trivial to reason about such dependencies, we have implemented our own \emph{job} 
scheduler.  We use MPI-2's one-sided communication library (via the communication module) to implement \emph{job}
stacks, similar to their shared memory counterparts. We choose to implement a stack because it suits better
future logic, we need to execute the latest issued \emph{job} in order for the get() not to block indefinitely
in recursive algorithms.  Using one-sided communication, only the issuer needs to copy the functor object to
the workers stack, as in a shared memory environment.  Figure~\ref{fig:task_stack} 
shows how a \emph{job} stack is structured.  
Note that an entry is composed by the functor object, its arguments (they are considered one object) and the 
size of the entry. This is necessary since different functors and/or different arguments result in 
varying entry sizes.
Thus, the exact location of a \emph{job} is calculated using the stack head and functor object size values 
\footnote{functors and arguments are send/received as output/input archives, using boost.serialize library.}.
Moreover, at the beginning of the shared space, the size and current head values are stored, so all processes
can push \emph{jobs}.

\paragraph{}
Figure~\ref{fig:helloCF} shows a control flow graph for the master and worker processes.  
The master simply initializes
the futures environment and issues \emph{async} functions while executing user code.  At the end it finalizes 
the futures environment and calls the terminate routine from the scheduler.  The workers initialize 
the futures environment, which must happen collectively among all workers and master and then enter a
loop, looking for pending jobs in their stacks until their terminate routine returns true, in which case
they exit the loop, finalize again collectively with all other processes and exit the program, without ever
returning to the main function.  The scheduler is responsible for providing the functionality of the terminate
routines.  In our implementation the workers poll a local variable which they expose through the communication
module as a shared variable.  The master, when calling his terminate routine, will check the status of every 
worker.  A process can be either idle, busy or terminated.  Process status is again exposed by a shared variable
on each process.  The master will check the status of all the processes and if all of them are in idle status, he
will set the terminated flag to true on all of them.  If a process is still busy, meaning executes some \emph{job}
or has still pending \emph{jobs} in its stack, the master must wait till all jobs are finished and then set the 
terminated flags.

\paragraph{}
When running user code or a \emph{job} and an \emph{async} call is made, the process will address the scheduler in order
to get the id of the next available process and allocates enough space for the return value to be stored.
Then it asks the scheduler to send the job to the worker process.  In our implementation the scheduler 
pushes the job int the process' stack.  Our scheduler distributes \emph{jobs} in a round robin fashion 
(excluding the master process, which should run user code). 

\section{Extra Features}
\label{sect:extra_features}

\subsection{Additions to the User Interface}
\paragraph{}
Up to this point, the interface we have described includes only the very basic routines of the std futures interface.
Other futures interfaces, such as HPX and boost have enriched their interfaces with additional routines.  One routine
we found to be useful, is the \texttt{make\_future} routine.  In our library, the \texttt{make\_future} routine is used
to create a future variable and initialize it with a value.  This is useful in cases we would like to have a future
value but we already know the value it should hold, while we would like to use this variable at a later point of the code
as an actual future.

\paragraph{}
In section ~\ref{sect:scheduler} we described our scheduler module.  This modules mainly facilitates the necessary 
infrastructure to send \emph{job}s between processes asynchronously, but also it is responsible for making a decision
on how \emph{job}s will be distributed among these processes.  Although a simple scheduler policy, like Round Robin, 
can be sufficient for many scenarios, it is possible that a more elaborate work or data distribution scheme is required
for better performance.  Especially in a distributed environment, we would like to be able to distribute data in fashion
that takes advantage of data locality and/or avoid excessive data transmission through the network.  For this reason, we
have added a variation of the async function, \texttt{async\_on}, which is identical to an \emph{async} with the addition of
the target process id.  The \texttt{async\_on} function still makes use of the scheduler infrastructure the same way
the default \emph{async}, it simply skips the step where the scheduler decides which process will receive the new 
\emph{job}.

\begin{figure}[!ht]
\begin{lstlisting}
template <class T>
class future {
private:
	... //serialization routine omitted
    int ready_status;
    T data;
    int src_id, dst_id;
		Shared_pointer status_ptr;
		Shared_pointer data_ptr;
		int type_size;
		int data_size;
public:
		... //constructors and destructors omitted
    bool is_ready();
    T get();
};
\end{lstlisting}
\caption{The future object definition.}
\label{lst:future_object}
\end{figure}

\begin{figure}
\begin{subfigure}[b]{.6\textwidth}
\begin{minipage}{.48\textwidth}
\begin{lstlisting}
...
for(int i=0; i < N; i++) {
	res1[i] = async(stage1);
}

for(int i=0; i < N; i++) {
	res2[i] = 
		async(stage2, res1[i].get());
} 

for(int i=0; i < N; i++) {
	res3[i] = 
		async(stage3, res2[i].get());
} 
...
\end{lstlisting}
\caption{Pipeline without serilizable futures}
\label{lst:pipe_futures_a}
\end{minipage}
\end{subfigure}
\hfill
\begin{subfigure}[b]{.6\textwidth}
\begin{minipage}{.48\textwidth}

\begin{lstlisting}
...
for(int i=0; i < N; i++) {
	res1[i] = async(stage1);
}

for(int i=0; i < N; i++) {
	res2[i] = async(stage2, res1[i]);
} 

for(int i=0; i < N; i++) {
	res3[i] = async(stage3, res2[i]);
} 
...
\end{lstlisting}
\caption{Pipeline with serilizable futures}
\label{lst:pipe_futures_b}
\end{minipage}
\end{subfigure}
\caption{	Different implementations of a pipeline scheme using our futures .}
\end{figure}

\subsection{Future Serialization}
\paragraph{}
An important addition to our futures library, is the serialization of a futures object.  By serializing a future
object, we can practically pass it as an argument the \emph{async} function.  This is important, because this way
a future created on one process, can be transferred to another one, thus synchronization can take place on the 
worker process, which allows finer granularity when synchronizing task.  Consider the two examples in figures 
\ref{lst:pipe_futures_a} and \ref{lst:pipe_futures_b}, where we can observe a pipeline scheme implemented 
without future serialization and with future serialization.  In figure \ref{lst:pipe_futures_a} at pipeline
loop stage2, the master process will have to wait on res1[0], even if re1[n], where n > 0, is available before
res1[0].  This forces sequential issuing of the stage2 \emph{async}, which can limit performance and breaks the 
pipelining scheme.  Now consider figure \ref{lst:pipe_futures_b}, in this example each stage function will only
need to wait on its corresponding future, even if previous futures in the arrays res1 and res2 are not available,
a stage function can proceed normally its execution.  This time we have an accurate pipeline implementation.

\paragraph{}
In our implementation, a future object is defined as shown in \ref{lst:future_object}.
The variable \texttt{ready\_status} is the current
status of the future which is true if the value has been set or false otherwise.  \texttt{Data}, is a local storage for the
the future's value, once the future has been set by the remote process.  The variables \emph{src\_id} and \emph{dst\_id}
hold the id value of the owner of the future and the process that will set the future value respectively.  
The \texttt{data\_ptr} and \texttt{status\_ptr} variables are \texttt{Shared\_pointers} 
(see section \ref{sect:shared-memory-manager}),
which hold all the
information needed for the owner of the future to retrieve the data and the future status from his shared address space. 
Finally, \texttt{data\_size} and \texttt{type\_size} are the number of elements and the
type size of the data the future wraps around.  A future object can be trivially serialized by serializing each
of its member using the boost serialization library.  One however must be aware that when a new future is created
the Shared Memory Allocator module will first allocate the memory needed for the future's data and status in the 
shared memory segment of the original future owner.  This is done for performance reasons, since accessing local
variables costs considerably less than accessing remote ones, and the way the get method is implemented requires 
regular polling on the \texttt{status\_ptr} variable.  However, when we serialize a \texttt{Shared\_pointer} variable,
it will still point on the same memory, on the original owner.  This implies, that the new owner will have to 
access the \texttt{data\_ptr} and \texttt{status\_ptr} using the underlying communication library of our implementation.
The reason for this limitation is that changing the data's and status' location to another process, would require an 
update to all other processes that are associated with that future.  This would require significant synchronization and
communication. In practice, the original owner of the
future, will act as a proxy between the new future owner and the process that will set its value.  Also note that it is
possible for the user to still retrieve the future value from the original owner, or have it sent to multiple
processes, since the actual data will always reside on the same place for everyone.  This means that our future 
object behaves just like C+11's \texttt{shared\_future} object. 

