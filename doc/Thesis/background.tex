\chapter{Background}
\section{Parallel Computing}
\paragraph{}
	High performance computing is today strongly related with parallel programming.  On one end, computer
architectures have been developing parallel machines or netword configurations for clusters of machines
in order to increase performance, and on the other, researchers have been trying to develop programming
models that will allow programmers to develop or port efficiently their applications to these emerging 
technologies.  When developing parallel applications the two most dominant and widely used programming 
models are threads and message passing.  

\paragraph{}
Threads model is commonly used on shared memory machines, where 
the communication scheme would have one thread writing to a memory location
and another thread reading the data from that location.  This model does not require data to be
transfered among threads but can lead to race conditions when two threads try to access the same
data at the same time or, if a thread does not respect RAW and WAR dependencies. 
In order to ensure correct program execution,
the user must synchronize memory access by the threads using mutexes, semaphores, locks, barriers etc.
Correct synchronization has proven to be a daunting and error-prone task for programmers, and often 
synchronization bugs in application can be the cause for  
erroneous results, or even worse, deadlocks.  Pthreads and OpenMP~\cite{Dagum:1998:OIA:615255.615542}
 are two commonly used libraries
that are used to program threads on shared memory machines.  With Pthreads the user can create and
launch and threads, where each thread will have a specific work to do.  The library also offers 
a variaty of synchronization primitives such as locks, barriers, mutexes etc.  OpenMP offers
a higher abstraction level interface, where the programmer uses special \#pragmas to annotate
code sections that should be executed in parallel.  These pragmas can denote loops that should 
be run in parallel or even organize parallel work into tasks~\cite{Ayguade:2009:DOT:1512157.1512430},
while the library takes care of creating and launching threads.  However, the user is again responsible
for synchronizing data accesses.

\paragraph{}
In contrast with the threads model,  applications using the message passing model, use messages to share data
between different processes and also for synchronization.  The usual scheme requires a matching pair
of send and receive operations where both application will have to eventually block at some point until
the message has been received. Although the message passing model is considered more difficult to program 
than programming with threads on shared memory, it is easier to reason about data locality, thus can 
potentially achieve very good performance.  Moreover, message passing libraries are usually the only 
available option on large scale distributed machines, where different physical nodes do not share 
a global address space.  A drawback of most message passing implementations is that two-sided 
communication is required when exchanging messages.  This means that both sender and receiver must
take active part in the communication, which usually means that both need to block at some point,
until the message is sent/received.  

\paragraph{}
An alternative from the usual message passing two-sided communication model, 
is the one-sided communication model, where one process can remotely write
or read from the address space of another process, while the latter is not required 
to take active part in the transaction.  ARMCI~\cite{Nieplocha99armci:a} 
LAPI~\cite{Shah:1998:PEL:876880.879642} and MPI-2 provide library implementations of such 
one-sided communication interface. OpenSHMEM~\cite{Chapman:2010:IOS:2020373.2020375} is 
an effort to standardize the SHMEM. An attractive property of this model, is that communication can happen 
asynchronously, which also means however that the programmer needs to explicitly synchronize
processes as in the shared memory model, using barriers and fences.  

\paragraph{}
The emergence of the one-sided communication model has made it possible to develop libraries and languages
that follow the PGAS \emph(Partitioned Global Address Space) programming model.  In this model,
a virtual global address space to the programmer, when in fact, this address space is 
distributed among the different nodes or a logical partition dedicated to a single thread.  
This model tries again to exploit the benefits of the message passing's SIMD model while providing
an easy way to address data as in the shared memory models.  UPC, Chapel and Fortress are languages
that use the PGAS model and are built ontop of a one-sided communication library.  
Global Arrays~\cite{Nieplocha:2006:AAP:1125980.1125985} is also
an API that follows the PGAS model and is built ontop on ARMCI~\cite{Nieplocha99armci:a}. 

\paragraph{}
Because all of the previous models are either considered difficult to program or error prone, a lot of higher 
level programming models have been suggested in the literature, that are implemented on top of one of the 
previous, lower level, ones.  The concept of organizing parallel work in functions that can be run
concurrently has lead to the development of many task-based programming models
~\cite{Ayguade:2009:DOT:1512157.1512430, Blumofe95cilk:an} in the shared memory
environment and to similar models in distributed memory like Remote Procedure Calls (RPC)
~\cite{Saunders:2003:AAP:966049.781534,Beckman96tulip:a,Vadhiyar03gradsolve-}
or Remote Service Request (RSR) ~\cite{Foster96thenexus}.  Although this higher level
abstraction makes it easier to organize parallel code in tasks, it is still up to the 
programmer to explicitly synchronize data accesses between tasks, using barriers, etc.
To address and simplify the synchronization problems, a lot of systems have been suggested
in the literatture, that provide implicit synchronization.  In the scope of task based parallel
models, these systems usually require some sort of task memory footprint description from the 
programmer~\cite{Tzenakis:2012:BBD:2370036.2145864, Perez:2010:HTD:1810085.1810122}
and/or have the compiler statically infer dependendencies among tasks 
~\cite{Jenista:2011:OSO:1941553.1941563, Zakkak:2012:IDI:2370816.2370892}.
This scheme usually allows the programmer to describe the data-flow relations between different
parallel tasks, and an underlying runtime systems will explicitly synchronise them.  The drawback
here is that there is usually an additional overhead from the runtime system and/or the automatic
(dynamic or static) analysis used to automatically synchronise the code, is often conservative
in order to maintain correctness, which harms performance.

\section{Futures and Promises}
\label{sect:futures-promises}

\paragraph{}
Experience with parallel programming has shown that common synchronization techniques like
barriers do not scale well on massively parallelization machines ~\cite{4100352}, with thousands of workers.
One would like to use finer grain synchronization, but reasoning about the exact point an operation will complete
is virtually impossible in a parallel environment.  An alternative is to use asynchronous programming models, which
allows the programmer to write programs where a thread or process can be oblivious to what actions the other threads/
processes are doing, while he can still retrieve the results of concurrent work and produce the correct result.

\paragraph{}
The futures (or promises) model is such an asynchronous programming model.  A future is a special variable which may 
or may not have a value at the time that it is referenced in program.  Usually a future is coupled with a promise.  
A promise is a special construct that is associated with a future and can be used by another thread or process to 
set the value of the future variable.  Usually, the future is used only to read the variable value, while the 
promise is used to write to the same variable, thus defining a data-flow relation between different threads/processes.
The promise construct is often hidden from the programmer, where instead he will have to declare a callable object
(function, functor, etc) and the library will offer a mechanism to use this callable object to set the future through 
the promise, after executing the user's callable object.  Such is the use of the \emph{async} function in the C++11 
standard, where the user can issue a function or functor object and retrieve a future object using the \emph{async} call.
The async will be run by a thread, and the return value of the function or functor will be used to set the future object 
associated with the \emph{async} call.   

\begin{figure}[!ht]
\includegraphics[width=\columnwidth]{figures/futures_blocking}
\caption{
The futures execution model of the blocking schematics.}
\label{fig:futures_blocking}
\end{figure}

An important design decision for any futures implementation, is what happens when a future is referenced, while its value
is not yet available.  A common choice, is to have the caller block until it the future value is resolved or implicitly
try to resolve the future at the reference time (as with lazy evaluation schemes).  Figure \ref{fig:futures_blocking} 
shows the execution model of the blocking scheme.  The green color is the time a thread spends doing useful computation, 
while the red color is the idle time a thread spends on waiting for the result of the future. This is the scheme used by C++11, 
an alternative is the Scala future implementation~\cite{Scala:Futures}, where the user can set a callable object to be
called when the future will be set, or if the future throws an exception (failure), using the callback mechanism.  This scheme
has the benefit that there will be no blocking at any point of the code, allowing true asynchronous execution.  The C++11
standard, as most blocking future implementations, offer the option to ask whether a future is ready before referencing its 
value, in order to avoid any blocking if possible. 

\begin{figure}[!ht]
\begin{lstlisting}
int	fibonacci(int n) {
	if(n == 0) return 0;
	if(n == 1) return 1;
	return fibonacci(n-1) + fibonacci(n-2);
}
\end{lstlisting}
\caption{A sequential fibonacci implementation}
\label{lst:fib_seq}
\end{figure}

\begin{figure}[!ht]
\begin{lstlisting}
int	fibonacci(int n) {
	if(n == 0) return 0;
	if(n == 1) return 1;
	future<int> fib1 = async(fibonacci, n-1);
	future<int> fib2 = async(fibonacci, n-2);
	return fib1.get() + fib2.get();
}
\end{lstlisting}
\caption{A fibonacci implementation using the C++11 futures interface}
\label{lst:fib_futures}
\end{figure}

\paragraph{}
Other than the asynchronous execution model they use, we believe that another attractive property of the futures model 
is that they can be used to compose easy to use interfaces, with higher level abstractions that the programmer can use 
in order to develop parallel applications. 

Other than their asynchronous execution model, we believe that futures offer an easily programmable and expressive 
user interface.
As a motivation to the reader, we present in figure \ref{lst:fib_futures} a
Fibonacci function implementation, using the C++11 standard threads library~\cite{CPP:Threads} future interface.  Figure
\ref{lst:fib_seq} also shows the sequential equivalent.  The parallel version simply requires the recursive calls 
to be issued using the \emph{async} function, and the use of the get method on the future objects in order to retrieve
the return values, of the recursive calls.  Note, that the call to the get method here is blocking.  


\section{MPI one-sided communication}
\label{sect:mpi-one-sided}
\paragraph{}
One of the most controversial features of MPI-2 is it's one-sided communication.  Although PGAS programming
models and languages have become widely accepted for developing code in large scale machines, programmers
consider the MPI one-sided communication interface to be generally difficult to understand and use.  In this
section we try to familiarize the reader with the main concepts of the interface.  

\paragraph{}
In order to perform remote access operations on some data, this data, residing on one process, needs to be exposed to 
the other processes, through an \texttt{MPI\_Window} object.
Thus, all processes need to create an \texttt{MPI\_Window} that will expose
part of their local address space to all other processes.  
\texttt{MPI\_Windows} are created using the \texttt{MPI\_Win\_create} function.
Figure \ref{lst:ping-pong-mpi} shows how a window can be created on line 13. 
This functions requires a pointer to a local address space, that was allocated with \texttt{MPI\_Alloc\_mem}.
The rest of the arguments to \texttt{MPI\_Win\_create} are the number of elements and type size of the data to be shared,
an MPI info flag, an MPI communicator and the window. 
This is a collective operation over a group of MPI processes.  Each process can expose different size of data (or none)
to the window. Note that only the processes in the group will be able
to perform a remote operation on the created \texttt{MPI\_Window}.

\paragraph{}
The two main operations that can be performed are \texttt{MPI\_Put} and \texttt{MPI\_Get}, 
which allow a process to remotely write and read some data respectively.  In the example in figure \ref{lst:ping-pong-mpi}
there are multiple calls to both \texttt{MPI\_Put} and \texttt{MPI\_Get}. A buffer that data will be written from or to
has to be provided.  The size of the local buffer must be defined as well as its MPI datatype.  The same must be done
for the remote buffer on the process on the other end of the communication.  In order to target the correct remote buffer,
the id of the target process and the window related to that buffer must also be provided.
Another operation available is the \texttt{MPI\_Accumulate}, that can be used to apply some action on the data 
that is remotely read and the local data on the process.  An operation must also be supplied to this function.    

\begin{figure}[!ht]
\begin{lstlisting}
#include <stdio.h>
#include "mpi.h"

int main(int argc, char** argv) {
	int rank, procs, msg_size;
	char *shared_buff, *message;
	MPI_Win win;

	MPI_Init(&argc, &argv);
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);

	MPI_Alloc_mem(sizeof(char)*msg_size, MPI_INFO_NULL, &message);
	MPI_Win_create(	message, msg_size, sizeof(int), 
									MPI_INFO_NULL, MPI_COMM_WORLD, &win);

	if(rank == 0) { //master code
		message = strdub("ping");
		//epoch 1
		MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 1, 0, win);
		MPI_Put(message, msg_size, MPI_CHAR, 0, 1, msg_size, MPI_CHAR, win);
		MPI_Win_unlock(target_rank, win);
		//epoch 2
		MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 1, 0, win);
		MPI_Get(shared_buff, msg_size, MPI_CHAR, 0, 1, msg_size, MPI_CHAR, win);
		MPI_Win_unlock(target_rank, win);
	}
	else { //worker code
		message = strdub("pong");
		//epoch 1
		MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);
		MPI_Get(shared_buff, msg_size, MPI_CHAR, 0, 0, msg_size, MPI_CHAR, win);
		MPI_Win_unlock(target_rank, win);
		//epoch 2
		MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);
		MPI_Put(message, msg_size, MPI_CHAR, 0, 0, msg_size, MPI_CHAR, win);
		MPI_Win_unlock(target_rank, win);
	}

	MPI_Win_free(&win);
	MPI_Free_mem(message);
	MPI_Finalize();
}
\end{lstlisting}
\caption{Simple "ping pong" example using MPI's one-sided communication interface}
\label{lst:ping-pong-mpi}
\end{figure}

\paragraph{}
In contrast to the two-sided communication interface, in the one-sided interface, get and put operation need not 
be paired and non-blocking. Synchronizing processes that perform these remote operations must be explicitly
done by the programmer.  Synchronization in the MPI one-sided communication interface is achieved using "epochs",
that define the start and end of an operation.  All one-sided operations must happen in one "epoch".  MPI provides
two different ways to define "epochs", called \emph{active target} and \emph{passive target}.

\paragraph{}
In the \emph{active target} mode both processes are required to take part in the synchronization.  The programmer 
need to declare the beginning and end of an "epoch" in the origin process, by explicitly calling 
\texttt{MPI\_Win\_start} and \texttt{MPI\_Win\_complete}, respectively.
On the target process, \texttt{MPI\_Win\_post/wait} must be used to declare the beginning and end of the "epoch".
\texttt{MPI\_Win\_start} needs
to be paired with an \texttt{MPI\_Win\_post} and \texttt{MPI\_Win\_complete} must be paired with an \texttt{MPI\_Win\_wait}.  
Moreover, an "epoch" can be defined by using a pair of \texttt{MPI\_Win\_fence} calls to declare the start and end of the 
"epoch".  This function is used for collectively synchronizing remote operations.  
All these functions require an MPI\_Window to be provided as an argument. 

\paragraph{}
The \emph{passive target} mode requires only the origin process to define the start and end of an "epoch", by using 
\texttt{MPI\_Win\_lock/unlock} respectively.  Again, the window on which the operation is performed is required to be passed as
an argument along with the rank of the target process.  An \texttt{MPI\_Win\_lock/unlock} can be either shared or exclusive.
A shared lock allows or concurrent operations to take place in the same "epoch", while the exclusive will force them
to happen in different "epochs".  However, note that concurrent conflicting accesses to the same MPI\_Window can be 
erroneous, and MPI locks are not to be confused with mutual exclusion schemes.

\paragraph{}
In the example in figure \ref{lst:ping-pong-mpi} we use the \emph{passive target} scheme to define the "epoch".
Each remote operation is surrounded by a pair of \texttt{MPI\_Win\_lock}, \texttt{MPI\_Win\_unlock}.
The master process first will send
the ping message to the worker process.   This is done in one matching "epoch" on both processes.  At the first call 
of \texttt{MPI\_Win\_unlock} at line 21, the master process is required to have finished sending the data to the worker
process.   Respectively, at line 32, the worker process will need to receive the data before moving on.  Note that this
can happen asynchronously.  In the second "epoch" the worker will send a pong message to the master, in the same fashion.

