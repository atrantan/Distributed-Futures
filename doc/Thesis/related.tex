\chapter{Related Work}
\label{chap:related}

\section{MPI one-sided communication Evaluation}
\paragraph{}
Dinan et al~\cite{Dinan:2012:SGA:2357496.2358660}
have implemented the Global Arrays (GA)\cite{Nieplocha:2006:AAP:1125980.1125985}, a PGAS model, over the one-sided
communication interface of MPI.  In their work they ported GA's low-level ARMCI~\cite{Nieplocha99armci:a} one-sided
communication librari using the MPI API and compared it to ARMCI.  
Although, they succesfully delivered a high-performance runtime, they 
are critical on both interface usability and performance of MPI one-sided interface.  Bonachea in his report
\cite{Bonachea:mpi2} also supports that the MPI one-sided interface is not fit to be used for the implementation 
of PGAS languages.  There are however examples~\cite{A_hydra-mpi:an, Cui:2010:SES:1884643.1884646}
where MPI's one-sided interface has been succesfully used to implement high-performance applications.  

\section{Distributed Futures Implementations}
\paragraph{}
High Performance ParalleX (HPX)~\cite{HPX:TOBE} is a parallel runtime system 
implementation of the ParalleX\cite{Kaiser:2009:PAP:1678991.1679815} 
execution model.  One of ParalleX's many features is the futures synchronization model.
The adopted futures interface is similar to the C++11 standard library one and is available for both shared and distributed
memory.  The model offers additional abstractions to the futures interface, that can be used to describe data-flow relations
and asynchronous computations.
In contrast with our work, it does not use an MPI library for communication, but a different batch system.  

\section{Other asynchronous distributed systems}
\paragraph{}
Other high-performance systems that support Remote Method Invocation (RMI), RSR and RPC share similar specifications
with our runtime system, regarding asynchronous execution.

\paragraph{}
ARMI\cite{Saunders:2003:AAP:966049.781534} is a low-level hybrid (using both threads and message passing) communication library, 
which supports RMI.  In ARMI, objects are shared between threads and processes, but requires manually setting the aggregation
factor in order to have an object's data effectively distributed on processes that do not share a common address space.  On such
case, method calls of the object are done using the library's RMI primitives.  These primitives are implemented on top of MPI and
although they share similar asynchronous charasteristics with our system's implementation, they emulate asynchrony by polling 
at certain time intervals.

\paragraph{}
Tulip\cite{Beckman96tulip:a} is another object-parallel system.  It provides implementations of remote access put/get and RPC 
primitives
over different hardware setups and requires a compiler to create the handlers used in RPC and Active Messages.  In some cases, 
these primitives are implemented using the MPI library and polling for messages, if DMA is not available for communication
between processes.

\paragraph{}
Charm++~\cite{Kale93charm++:a} is an parallel object-oriented extension to the C++ language.  It is based on the 
\emph{Actors}~\cite{conf/icpp/HouckA92} but differentiates sequential and parallel objects.  
It uses a Message driven execution model,
different from the traditional send/receive pairing, where computations begin when a message is received.  The parallel,
work unit in Charm++ is called a \emph{chares} and different \emph{chares} can communicate between themselves.  Instead
of RPC, it uses a futures implementation that provides the same interface for local and remote invocation, in order to have overlapping communication and computation.  This implementation however is not based on MPI or another one-sided 
communication interface.

\paragraph{}
The RSR sheme from Nexus\cite{Foster96thenexus} is similar to the RPC. The user needs to define a handler for the 
RSR that is going to be run remotely, and the data the handler will operate on.  The underlying system will decide
on the mechanism used that the data will be communicated.  Nexus offers a variaty of methods to achieve asynchronous
commonucation in order to remotely execute RSR handlers, depending on available OS and/or hardware.  
We will discuss these different techniques shortly, at the end of this section. 

\paragraph{}
Active-Messages is another communication model, where data that is transfered between processes is paired with 
a handler, which is an action that is performed upon the arrival of data on a process.  This scheme shares some common
asynchronous characteristics with the RPC model. 
AMMPI\cite{Bonachea:ammpi} is an Active-Messages implementation over MPI two-sided communication
interface and LAPI~\cite{Shah:1998:PEL:876880.879642} is a low-level communication library, that offers an interface
similar to Active Messages.  

\paragraph{}
Most of these systems require asynchronous communcation to be effective.  There is a number of known solutions as 
to how to implement such systems in the literature.  The two most commonly used methods are polling for work requests
~\cite{Beckman96tulip:a, Saunders:2003:AAP:966049.781534, Foster96thenexus, vonEicken:1992:AMM:146628.140382} 
and hardware interrupts.
Polling would require a worker process to poll for incoming messages/work at certain
time intervals.   Extra care must be taken to define the polling period, since if polling happens too often, it can
dominate computation, but infrequent polling could render the system unable handle request in time. 
~\cite{Saunders:2003:AAP:966049.781534, Shah:1998:PEL:876880.879642}.
Alternatively, a hardware interrupt could be sent to notify a process of an incoming message.  This method however,
is avoided because interrupts have to go through the OS, which has a significant cost. 
~\cite{Saunders:2003:AAP:966049.781534, Shah:1998:PEL:876880.879642, Foster96thenexus, vonEicken:1992:AMM:146628.140382}.
The Nexus system ~\cite{Foster96thenexus}, also suggests dedicating threads only for communication.  These threads can 
either probe for pending messages or block (depending on the underlying communication library and OS capabilities).
How responsive this implementation can be depends on the thread implementation and OS (for example if the OS supports
priorities).  A detailed discussion and comparison between using threads for communication versus probing or interrupts
can be found in~\cite{Foster96thenexus}.

\paragraph{}
In our implementation, we use none of these methods, instead we use MPI's one-sided communication interface.  The benefit
of using a one-sided communication interface, lies in the fact that we can have real asynchronous execution.  In contrast 
with polling, the the system can react without any delay (polling period), while it will not suffer from costly interrupts
or the extra overhead and reponse delay of having a thread running, as we discussed above.  However, synchronization in such
a system can become a serious performance problem, as with shared memory models (fences, barries mutexes).




